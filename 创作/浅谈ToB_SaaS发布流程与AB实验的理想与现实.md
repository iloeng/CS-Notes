# 浅谈 ToB SaaS 发布流程与 AB 实验的「理想与现实」

## 背景：一次关于“代码成熟度”的讨论

最近群里关于发布流程的讨论很有意思。作为这次讨论的发起方，我当时的核心困惑在于：**AB 实验（验证逻辑）和全量上线（Release）的先后顺序**。

*   **我的初始观点（从第一性原理出发）**：坚持代码成熟度应小于上线范围。风险高的代码（未经验证的 AB 代码）不应该直接全量发布到所有 Pod。理想路径是：`本地自测 -> 开启AB（小流量验证） -> 验证无误 -> 全量上线`。
*   **团队的反馈（结合业务现状）**：文东和少晨指出了 ToB SaaS 的现实约束。目前的路径是：`本地自测 -> 全量发布（开关关闭） -> 线上集成测试（Solid Verification） -> 开启AB开关 -> 全量开启`。

经过深入讨论，我意识到这看似是一个流程顺序问题，本质上是 **ToC 海量业务的标准范式** 与 **ToB 小客户 SaaS 架构约束** 之间的碰撞。

## 核心冲突：为什么不能直接照搬“站内”模式？

在字节内部（“站内”），我们习惯了抖音、头条这种航母级的研发模式。但在 ToB SaaS 场景下，环境变了，物理定律也跟着变了。

### 1. 容灾的“物理学”差异：从 1000 到 2

*   **站内（ToC）**：如果你有 $$N=1000$$ 个 Pod，拿 1 个 Pod 做金丝雀发布（Canary）或 AB 测试，风险敞口（Blast Radius）是 $$1/1000 = 0.1\%$$。挂了就挂了，熔断摘除即可，用户几乎无感。
*   **ToB SaaS**：我们目前的架构是 **多租户共享集群（Multi-tenant Shared Cluster）**。
    *   虽然 `rec retriever` 只是一个 CPU 服务，成本并不高，但由于 ToB 整体流量较小，为了资源效率（不做无谓浪费），往往一个共享集群可能只配置 $$N=2$$ 个 Pod（满足最小高可用）。
    *   **高风险敞口**：如果我拿 1 个 Pod 跑未经验证的 AB 代码，一旦 Core Dump 或 OOM，整个集群瞬间**损失 50% 运力**。
    *   **邻居干扰（Noisy Neighbor）**：更严重的是，因为是共享集群，客户 A 的 AB 实验如果搞挂了 Pod，完全没参与实验的客户 B、C、D 也会受到波及。这种“一人吃药，全家挂水”的风险是平台方无法接受的。

这就是为什么文东强调“集成测试是为了测你的分支代码是不是被运行了”——我们必须在不开流量的情况下，先确认代码逻辑本身是安全的。

### 2. 流量的“统计学”陷阱

AB 实验的灵魂是**统计显著性**。
*   **站内**：百万 QPS，几分钟就能跑出置信度。
*   **ToB SaaS**：很多小客户可能只有 10 QPS 甚至更低。
    *   **物理手段分流（按 Pod 分流）**：如果是 2 个 Pod，你要么 0%，要么 50%。对于小流量客户，50% 的流量切分粒度太粗，风险无法控制。
    *   **逻辑手段分流（按 UserID 分流）**：虽然可以做到 1% 流量，但因为 ToB 流量盘子小，跑出一置信结果可能需要很久。这就导致“为了验证代码成熟度”而挂着一个长周期的实验，反而拖慢了迭代效率。

## 破局思路：解耦「部署」与「发布」

既然不能依赖“大数定律”来规避风险，且受限于“共享集群”的连带责任，我们的发布策略必须转向**确定性验证**。

目前的方案（Feature Flag）其实是一个非常经典的 **Decoupling Deployment from Release**（解耦部署与发布）的实践：

1.  **Merge & Deploy (物理上线)**：
    *   *动作*：代码合并进 master，触发 CI/CD 流程，全量部署到所有 Pod。
    *   *逻辑*：此时 Feature Flag 默认为 **关闭** 状态。这意味着**主干代码的质量**必须由 Code Review 和离线测试（UT/IT）严格把关，因为一旦代码包含有副作用（Side Effect）的逻辑（如全局变量初始化），即使开关没开，也可能导致事故。
    *   *对应群聊*：“代码是进 master 的，配置在 prod 没开”

2.  **Verification (线上验证)**：在线上环境进行“确定性”测试。
    *   *动作*：使用特定的 Test Account (灰度账号) 命中特定逻辑。
    *   *隐忧*：**目前的痛点在于不够 Solid**。如果仅仅依赖灰度账号的 Mock 流量，往往覆盖不了真实场景的复杂性（数据分布、并发情况等）。正如我担心的，这导致这层“盾牌”可能漏防，让 Release 阶段依然面临较大风险。
    *   *对应群聊*：“集成测试就是为了测你的分支代码是不是被运行了”。

3.  **Logical Release (逻辑上线)**：通过配置下发，逐步放开流量。
    *   *动作*：`0% -> AB Test (特定流量) -> 100% (全量开启)`。
    *   *风险*：此时才真正开始承担业务风险。由于 Verification 阶段可能存在覆盖不全的问题（不够 Solid），这一步的“开关开启”实际上仍然是一次**高风险操作**。

## 总结与反思

### 1. 验证必须真的 "Solid"
这其实是我们目前流程中最大的软肋。如果 Step 2 的 Verification 只是跑几个 Mock Case，那其实并没有真正排除风险，只是把“爆炸”推迟到了 Step 3 开开关的那一刻。
*   **改进方向**：不能只依赖灰度账号的 Mock 流量。我们需要引入 **Traffic Replay（流量回放）** 或 **Shadow Mode（影子模式）**，用真实的（只读）流量来验证新逻辑，才能让 Verification 这一步真正立得住。

### 2. “钞能力”是很好的解法
这也引发了我对成本和架构的思考。正如群里讨论的，`rec retriever` 本质上是一个成本很低的 CPU 服务。我们之所以维持 $$N=2$$，更多是因为“流量小所以配得少”，而非“付不起钱”。
*   **打破思维定势**：既然成本不高，我们完全可以通过**增加冗余**来解决物理层的脆弱性。
*   **具体解法**：直接把集群的 Pod 数量从 2 提升到 4 或更多。虽然流量不需要这么多资源，但为了**稳定性（Availability）**和**隔离性（Isolation）**付费是值得的。
*   **发布策略升级**：有了冗余资源，我们甚至可以在发布时临时扩容一个“灰度 Pod”进行蓝绿验证，验证完后再杀掉。这样既保证了安全，又利用了成本低的优势，从根本上缓解了“发布焦虑”。

### 3. 研发效能的 Gap 与“钞能力”的互补
正因为目前的 Verification 还可以做得更 Solid（流量回放建设成本高），所以“钞能力”显得尤为重要。
*   **互补逻辑**：在 Verification 还没进化到完美之前，**资源冗余（临时扩容灰度 Pod）** 是我们最可靠的兜底。
*   **未来方向**：短期靠堆资源保平安（用蓝绿部署隔离风险），长期靠流量回放提效能（在 Verification 阶段拦截 Bug）。

**一句话总结**：
在 ToB SaaS 领域，在缺乏 ToC 那种“无限弹药”的掩护下，我们目前的 **Feature Flag + Mock Verification** 还是不够的。在流量回放等 **Solid Verification** 手段就位前，不妨大胆一点，利用服务低成本的优势，**用资源（临时扩容）换安全**。
